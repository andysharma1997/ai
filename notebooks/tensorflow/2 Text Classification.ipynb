{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, \n",
    "                                                        value = word_index[\"<PAD>\"], \n",
    "                                                        padding = 'post', \n",
    "                                                        maxlen = 256)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/absin/Documents/dev/sentenceSimilarity/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation = tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /home/absin/Documents/dev/sentenceSimilarity/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      " - 1s - loss: 0.6919 - acc: 0.5893 - val_loss: 0.6903 - val_acc: 0.6587\n",
      "Epoch 2/40\n",
      " - 1s - loss: 0.6872 - acc: 0.7048 - val_loss: 0.6838 - val_acc: 0.7330\n",
      "Epoch 3/40\n",
      " - 1s - loss: 0.6761 - acc: 0.7462 - val_loss: 0.6691 - val_acc: 0.7475\n",
      "Epoch 4/40\n",
      " - 1s - loss: 0.6554 - acc: 0.7540 - val_loss: 0.6451 - val_acc: 0.7509\n",
      "Epoch 5/40\n",
      " - 1s - loss: 0.6240 - acc: 0.7818 - val_loss: 0.6118 - val_acc: 0.7772\n",
      "Epoch 6/40\n",
      " - 0s - loss: 0.5832 - acc: 0.8035 - val_loss: 0.5720 - val_acc: 0.7961\n",
      "Epoch 7/40\n",
      " - 1s - loss: 0.5364 - acc: 0.8187 - val_loss: 0.5277 - val_acc: 0.8139\n",
      "Epoch 8/40\n",
      " - 1s - loss: 0.4888 - acc: 0.8402 - val_loss: 0.4861 - val_acc: 0.8279\n",
      "Epoch 9/40\n",
      " - 0s - loss: 0.4440 - acc: 0.8552 - val_loss: 0.4482 - val_acc: 0.8400\n",
      "Epoch 10/40\n",
      " - 1s - loss: 0.4041 - acc: 0.8689 - val_loss: 0.4167 - val_acc: 0.8494\n",
      "Epoch 11/40\n",
      " - 1s - loss: 0.3704 - acc: 0.8777 - val_loss: 0.3906 - val_acc: 0.8572\n",
      "Epoch 12/40\n",
      " - 1s - loss: 0.3419 - acc: 0.8863 - val_loss: 0.3706 - val_acc: 0.8594\n",
      "Epoch 13/40\n",
      " - 1s - loss: 0.3188 - acc: 0.8930 - val_loss: 0.3529 - val_acc: 0.8676\n",
      "Epoch 14/40\n",
      " - 1s - loss: 0.2980 - acc: 0.8983 - val_loss: 0.3401 - val_acc: 0.8703\n",
      "Epoch 15/40\n",
      " - 1s - loss: 0.2808 - acc: 0.9035 - val_loss: 0.3294 - val_acc: 0.8744\n",
      "Epoch 16/40\n",
      " - 1s - loss: 0.2653 - acc: 0.9082 - val_loss: 0.3205 - val_acc: 0.8747\n",
      "Epoch 17/40\n",
      " - 1s - loss: 0.2511 - acc: 0.9131 - val_loss: 0.3131 - val_acc: 0.8769\n",
      "Epoch 18/40\n",
      " - 1s - loss: 0.2385 - acc: 0.9181 - val_loss: 0.3068 - val_acc: 0.8798\n",
      "Epoch 19/40\n",
      " - 1s - loss: 0.2272 - acc: 0.9207 - val_loss: 0.3014 - val_acc: 0.8808\n",
      "Epoch 20/40\n",
      " - 1s - loss: 0.2170 - acc: 0.9241 - val_loss: 0.2977 - val_acc: 0.8812\n",
      "Epoch 21/40\n",
      " - 0s - loss: 0.2067 - acc: 0.9297 - val_loss: 0.2944 - val_acc: 0.8820\n",
      "Epoch 22/40\n",
      " - 1s - loss: 0.1980 - acc: 0.9313 - val_loss: 0.2916 - val_acc: 0.8829\n",
      "Epoch 23/40\n",
      " - 1s - loss: 0.1892 - acc: 0.9363 - val_loss: 0.2902 - val_acc: 0.8838\n",
      "Epoch 24/40\n",
      " - 1s - loss: 0.1817 - acc: 0.9401 - val_loss: 0.2883 - val_acc: 0.8848\n",
      "Epoch 25/40\n",
      " - 1s - loss: 0.1739 - acc: 0.9438 - val_loss: 0.2866 - val_acc: 0.8841\n",
      "Epoch 26/40\n",
      " - 1s - loss: 0.1670 - acc: 0.9462 - val_loss: 0.2868 - val_acc: 0.8831\n",
      "Epoch 27/40\n",
      " - 1s - loss: 0.1605 - acc: 0.9491 - val_loss: 0.2860 - val_acc: 0.8842\n",
      "Epoch 28/40\n",
      " - 0s - loss: 0.1542 - acc: 0.9523 - val_loss: 0.2861 - val_acc: 0.8851\n",
      "Epoch 29/40\n",
      " - 1s - loss: 0.1487 - acc: 0.9555 - val_loss: 0.2875 - val_acc: 0.8839\n",
      "Epoch 30/40\n",
      " - 1s - loss: 0.1430 - acc: 0.9562 - val_loss: 0.2866 - val_acc: 0.8858\n",
      "Epoch 31/40\n",
      " - 1s - loss: 0.1371 - acc: 0.9599 - val_loss: 0.2874 - val_acc: 0.8865\n",
      "Epoch 32/40\n",
      " - 0s - loss: 0.1319 - acc: 0.9613 - val_loss: 0.2886 - val_acc: 0.8862\n",
      "Epoch 33/40\n",
      " - 0s - loss: 0.1268 - acc: 0.9633 - val_loss: 0.2905 - val_acc: 0.8855\n",
      "Epoch 34/40\n",
      " - 1s - loss: 0.1223 - acc: 0.9652 - val_loss: 0.2923 - val_acc: 0.8856\n",
      "Epoch 35/40\n",
      " - 1s - loss: 0.1181 - acc: 0.9659 - val_loss: 0.2945 - val_acc: 0.8855\n",
      "Epoch 36/40\n",
      " - 1s - loss: 0.1136 - acc: 0.9688 - val_loss: 0.2960 - val_acc: 0.8854\n",
      "Epoch 37/40\n",
      " - 1s - loss: 0.1092 - acc: 0.9694 - val_loss: 0.2983 - val_acc: 0.8856\n",
      "Epoch 38/40\n",
      " - 1s - loss: 0.1052 - acc: 0.9708 - val_loss: 0.3015 - val_acc: 0.8832\n",
      "Epoch 39/40\n",
      " - 0s - loss: 0.1020 - acc: 0.9719 - val_loss: 0.3047 - val_acc: 0.8827\n",
      "Epoch 40/40\n",
      " - 0s - loss: 0.0979 - acc: 0.9740 - val_loss: 0.3072 - val_acc: 0.8834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff78f7bbc88>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(partial_x_train,partial_y_train, epochs = 40, batch_size = 512, validation_data = (x_val, y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 17us/sample - loss: 0.3279 - acc: 0.8719\n",
      "[0.32789759624004367, 0.87188]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
